{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a257f82c",
   "metadata": {
    "papermill": {
     "duration": 0.005224,
     "end_time": "2025-04-20T16:03:59.214004",
     "exception": false,
     "start_time": "2025-04-20T16:03:59.208780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3775a97a",
   "metadata": {
    "papermill": {
     "duration": 0.003968,
     "end_time": "2025-04-20T16:03:59.222494",
     "exception": false,
     "start_time": "2025-04-20T16:03:59.218526",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ****Install Dependencies****\n",
    "\n",
    "Uninstalls conflicting packages and installs required versions of google-genai, chromadb, and protobuf to ensure compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf4c8da1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:03:59.232169Z",
     "iopub.status.busy": "2025-04-20T16:03:59.231852Z",
     "iopub.status.idle": "2025-04-20T16:04:45.588946Z",
     "shell.execute_reply": "2025-04-20T16:04:45.587760Z"
    },
    "papermill": {
     "duration": 46.364163,
     "end_time": "2025-04-20T16:04:45.590825",
     "exception": false,
     "start_time": "2025-04-20T16:03:59.226662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping kfp as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "jupyterlab-lsp 3.10.2 requires jupyterlab<4.0.0a0,>=3.1.0, which is not installed.\r\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\r\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "pandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "google-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "opentelemetry-proto 1.32.1 requires protobuf<6.0,>=5.0, but you have protobuf 4.23.4 which is incompatible.\r\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 4.23.4 which is incompatible.\r\n",
      "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 4.23.4 which is incompatible.\r\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "pandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "google-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Uninstall unused conflicting packages and install required versions\n",
    "!pip uninstall -qqy jupyterlab kfp  \n",
    "!pip install -qU \"google-genai==1.7.0\" \"chromadb==0.6.3\"\n",
    "!pip install protobuf==4.23.4 --quiet  # ✅ Compatibility fix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e73bde",
   "metadata": {
    "papermill": {
     "duration": 0.005627,
     "end_time": "2025-04-20T16:04:45.602810",
     "exception": false,
     "start_time": "2025-04-20T16:04:45.597183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ****Import Generative AI Modules****\n",
    "Imports necessary classes and functions from the Google Generative AI SDK to interact with the Gemini model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e16fcd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:04:45.616612Z",
     "iopub.status.busy": "2025-04-20T16:04:45.615732Z",
     "iopub.status.idle": "2025-04-20T16:04:46.897713Z",
     "shell.execute_reply": "2025-04-20T16:04:46.896904Z"
    },
    "papermill": {
     "duration": 1.290322,
     "end_time": "2025-04-20T16:04:46.899041",
     "exception": false,
     "start_time": "2025-04-20T16:04:45.608719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Import all necessary modules\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "genai.__version__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2914a132",
   "metadata": {
    "papermill": {
     "duration": 0.00613,
     "end_time": "2025-04-20T16:04:46.911907",
     "exception": false,
     "start_time": "2025-04-20T16:04:46.905777",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ****Import Vector Store Tools****\n",
    "Loads ChromaDB for handling document embeddings and vector database operations, along with display and OS utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc61466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:04:46.925246Z",
     "iopub.status.busy": "2025-04-20T16:04:46.924731Z",
     "iopub.status.idle": "2025-04-20T16:04:47.697667Z",
     "shell.execute_reply": "2025-04-20T16:04:47.696917Z"
    },
    "papermill": {
     "duration": 0.781408,
     "end_time": "2025-04-20T16:04:47.699204",
     "exception": false,
     "start_time": "2025-04-20T16:04:46.917796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "from IPython.display import display\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3060bf2",
   "metadata": {
    "papermill": {
     "duration": 0.005861,
     "end_time": "2025-04-20T16:04:47.711267",
     "exception": false,
     "start_time": "2025-04-20T16:04:47.705406",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " # ****Install Google Generative AI****\n",
    "Installs the google-generativeai package to enable interaction with Gemini models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2ad2c42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:04:47.724705Z",
     "iopub.status.busy": "2025-04-20T16:04:47.724203Z",
     "iopub.status.idle": "2025-04-20T16:04:52.277140Z",
     "shell.execute_reply": "2025-04-20T16:04:52.275888Z"
    },
    "papermill": {
     "duration": 4.561731,
     "end_time": "2025-04-20T16:04:52.278944",
     "exception": false,
     "start_time": "2025-04-20T16:04:47.717213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\r\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\r\n",
      "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (1.34.1)\r\n",
      "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.160.0)\r\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.27.0)\r\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.23.4)\r\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.1)\r\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.0)\r\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.67.0)\r\n",
      "Collecting protobuf (from google-generativeai)\r\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\r\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\r\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\r\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\r\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.1)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\r\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.70.0)\r\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.48.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\r\n",
      "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: protobuf\r\n",
      "  Attempting uninstall: protobuf\r\n",
      "    Found existing installation: protobuf 4.23.4\r\n",
      "    Uninstalling protobuf-4.23.4:\r\n",
      "      Successfully uninstalled protobuf-4.23.4\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "opentelemetry-proto 1.32.1 requires protobuf<6.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\r\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "pandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "google-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db517f48",
   "metadata": {
    "papermill": {
     "duration": 0.006576,
     "end_time": "2025-04-20T16:04:52.292459",
     "exception": false,
     "start_time": "2025-04-20T16:04:52.285883",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "****Load API Key Securely****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7ffd4b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:04:52.307301Z",
     "iopub.status.busy": "2025-04-20T16:04:52.306936Z",
     "iopub.status.idle": "2025-04-20T16:04:52.486392Z",
     "shell.execute_reply": "2025-04-20T16:04:52.485640Z"
    },
    "papermill": {
     "duration": 0.188612,
     "end_time": "2025-04-20T16:04:52.487954",
     "exception": false,
     "start_time": "2025-04-20T16:04:52.299342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a733d3a6",
   "metadata": {
    "papermill": {
     "duration": 0.006346,
     "end_time": "2025-04-20T16:04:52.501198",
     "exception": false,
     "start_time": "2025-04-20T16:04:52.494852",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ****Install PDF Processing Library****\n",
    "Installs PyMuPDF to extract text content from PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ec62a9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:04:52.516183Z",
     "iopub.status.busy": "2025-04-20T16:04:52.515840Z",
     "iopub.status.idle": "2025-04-20T16:04:58.352497Z",
     "shell.execute_reply": "2025-04-20T16:04:58.351487Z"
    },
    "papermill": {
     "duration": 5.84576,
     "end_time": "2025-04-20T16:04:58.354033",
     "exception": false,
     "start_time": "2025-04-20T16:04:52.508273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q PyMuPDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351917f",
   "metadata": {
    "papermill": {
     "duration": 0.006477,
     "end_time": "2025-04-20T16:04:58.367672",
     "exception": false,
     "start_time": "2025-04-20T16:04:58.361195",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ****Extract Text from PDF****\n",
    "Defines a function to extract full text from a PDF file using PyMuPDF and prints a preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ddaaffe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:04:58.383328Z",
     "iopub.status.busy": "2025-04-20T16:04:58.382527Z",
     "iopub.status.idle": "2025-04-20T16:04:58.964114Z",
     "shell.execute_reply": "2025-04-20T16:04:58.962979Z"
    },
    "papermill": {
     "duration": 0.591183,
     "end_time": "2025-04-20T16:04:58.965672",
     "exception": false,
     "start_time": "2025-04-20T16:04:58.374489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI with Large\n",
      "Language Models.\n",
      "Course Notes : July, 2023\n",
      "Generative AI, and LLMs specifically, is a General Purpose Technology that is useful for a variety of\n",
      "applications. \n",
      "\"LLMs can be, generally, thought of as a next word prediction model\"\n",
      "What is an LLM?\n",
      "LLMs are machine learning models that have learned from massive datasets of human-generated\n",
      "content, finding statistical patterns to replicate human-like abilities.\n",
      "Foundation models, also known as base models, have been trained on trillions of words for weeks or\n",
      "months using extensive compute power. These models have billions of parameters, which represent\n",
      "their memory and enable sophisticated tasks.\n",
      "Interacting with LLMs differs from traditional programming paradigms. Instead of formalized code\n",
      "syntax, you provide natural language prompts to the models.\n",
      "What is an LLM?\n",
      "Page 1\n",
      "What are the Use Cases for application of LLMs?\n",
      "Page 2\n",
      "What are Transformers? How was text generation done before Transformers? Transformer Architecture.\n",
      "Page 2\n",
      "How does a Transformer generate Text?\n",
      "Page 4\n",
      "What is a Prompt?\n",
      "Page 5\n",
      "Generative AI Project Life Cycle.\n",
      "Page 7\n",
      "How do you pre-train Large Language Models?\n",
      "Page 8\n",
      "Challenges with pre-training LLMs.\n",
      "Page 9\n",
      "What is the optimal configuration for pre-training LLMs?\n",
      "Page 11\n",
      "When is pre-training useful?\n",
      "Page 12\n",
      "Page 11\n",
      "LLMs are machine learning models that have learned from massive datasets of human-generated content, finding statistical patterns to replicate human-like abilities.\n",
      "Founda\n",
      "tion models, also known as base models, have been trained on trillions of words for weeks or months using extensive compute power. These models have billions of parameters, which represent their memory and enable sophisticated tasks.\n",
      "Interacting with LLMs differs from traditional programming paradigms. Instead of formalized code syntax, you provide natural language prompts to the models.\n",
      "When you pass a prompt to the model, it predicts the next words and generates a completion. This process is known as inference.\n",
      "PART 3\n",
      "RLHF & Application\n",
      "Abhinav Kimothi\n",
      "Keep Calm & Build AI.\n",
      "Course Notes : July, 2023\n",
      "GENERATIVE AI WITH LARGE LANGUAGE MODELS\n",
      "LLM\n",
      "Where is Ganymede located in the\n",
      "solar system?\n",
      "Where is Ganymede located in the\n",
      "solar system?\n",
      "Ganymede is a moon of Jupiter and is\n",
      "located in the solar system within the\n",
      "orbit of Jupiter\n",
      "CONTEXT WINDOW\n",
      "PROMPT\n",
      "COMPLETION\n",
      "MODEL\n",
      "THIS PROCESS IS CALLED 'INFERENCE'\n",
      "What are the Use Cases for LLMs?\n",
      "While Chatbots have emerged to become the most popular applications of LLMs, there are a variety of\n",
      "other tasks that LLMs can be used to accomplish - \n",
      "Writing - From essays to emails to reports and more\n",
      "Summarisation - Summarise long content into a meaningful shorter length\n",
      "Language Translation - Translate text from one language to the other\n",
      "Code - Translate natural language to machine code\n",
      "Information Retrieval - Retrieve specific information from text like names, locations, sentiment\n",
      "Augmented LLM - Power interactions with real world by providing information outside of LLM training\n",
      "TRANSFORMERS.\n",
      "How was text generation done before Transformers?\n",
      "The arrival of the transformer architecture in 2017, following the publication of the \n",
      "\"Attention is All You Need\" paper, revolutionised generative AI.\n",
      "Before the arrival of transformers, text generation tasks were accomplished by Recurrent Neural\n",
      "Networks (RNNs).\n",
      "The next word was predicted looking at the previous few words. The more the number of previous\n",
      "words, the larger was the computational requirement of the RNN.\n",
      "The prediction wasn't great. The reason was the design of looking only at a few previous words.\n",
      "Abhinav Kimothi\n",
      "Keep Calm & Build AI.\n",
      "Course Notes : July, 2023\n",
      "What is Attention?\n",
      "Transformers supersede all previous natural language architectures because of their ability to 'pay attention'\n",
      "HOMONYMS\n",
      "TRANSFORMERS ARE ABLE TO PAY ATTENTION TO THE MEANING OF THE WORDS\n",
      "I took my money to the bank.\n",
      "River Bank?\n",
      "Financial Bank?\n",
      "The teacher taught the student with the book\n",
      "Did\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = \"/kaggle/input/gen-ai-notes/GENERATIVE_AI_WITH_LARGE_LANGUAGE_MODELS_1712321353.pdf\"  # update with your actual file\n",
    "full_text = extract_text_from_pdf(pdf_path)\n",
    "print(full_text[:4000])  # Show a preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddb1eb4",
   "metadata": {
    "papermill": {
     "duration": 0.006783,
     "end_time": "2025-04-20T16:04:58.979647",
     "exception": false,
     "start_time": "2025-04-20T16:04:58.972864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ****Split Text into Chunks****\n",
    "Breaks the extracted text into smaller chunks (~500 tokens) to make it suitable for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f29223b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:04:58.995317Z",
     "iopub.status.busy": "2025-04-20T16:04:58.994718Z",
     "iopub.status.idle": "2025-04-20T16:04:59.003357Z",
     "shell.execute_reply": "2025-04-20T16:04:59.002440Z"
    },
    "papermill": {
     "duration": 0.017971,
     "end_time": "2025-04-20T16:04:59.004640",
     "exception": false,
     "start_time": "2025-04-20T16:04:58.986669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 101\n",
      "Sample chunk:\n",
      " Generative AI with Large. Language Models. Course Notes : July, 2023. Generative AI, and LLMs specifically, is a General Purpose Technology that is useful for a variety of. applications. \"LLMs can be, generally, thought of as a next word prediction model\". What is an LLM?. LLMs are machine learning models that have learned from massive datasets of human-generated. content, finding statistical patterns to replicate human-like abilities.\n"
     ]
    }
   ],
   "source": [
    "def split_text(text, max_tokens=500):\n",
    "    # Rough split by sentence or paragraph\n",
    "    import re\n",
    "    sentences = re.split(r'\\n|\\.\\s+', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) <= max_tokens:\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \". \"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "text_chunks = split_text(full_text)\n",
    "print(f\"Total Chunks: {len(text_chunks)}\")\n",
    "print(\"Sample chunk:\\n\", text_chunks[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51bf324",
   "metadata": {
    "papermill": {
     "duration": 0.00692,
     "end_time": "2025-04-20T16:04:59.018632",
     "exception": false,
     "start_time": "2025-04-20T16:04:59.011712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ****Configure Gemini API****\n",
    "Sets the API key to authenticate and enable the use of Gemini’s embedding and generation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60608daf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:04:59.034535Z",
     "iopub.status.busy": "2025-04-20T16:04:59.033633Z",
     "iopub.status.idle": "2025-04-20T16:05:00.456147Z",
     "shell.execute_reply": "2025-04-20T16:05:00.455193Z"
    },
    "papermill": {
     "duration": 1.432226,
     "end_time": "2025-04-20T16:05:00.457847",
     "exception": false,
     "start_time": "2025-04-20T16:04:59.025621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.generativeai import configure\n",
    "from google.generativeai import embed_content\n",
    "\n",
    "# Already done by you, but ensure this is here\n",
    "configure(api_key=GOOGLE_API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41d4c6",
   "metadata": {
    "papermill": {
     "duration": 0.006896,
     "end_time": "2025-04-20T16:05:00.472420",
     "exception": false,
     "start_time": "2025-04-20T16:05:00.465524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " # ****Generate Embeddings****\n",
    "Embeds each chunk of text using Gemini’s embedding model for storage and similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bffdc821",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:05:00.488714Z",
     "iopub.status.busy": "2025-04-20T16:05:00.487550Z",
     "iopub.status.idle": "2025-04-20T16:05:30.470103Z",
     "shell.execute_reply": "2025-04-20T16:05:30.469144Z"
    },
    "papermill": {
     "duration": 29.992366,
     "end_time": "2025-04-20T16:05:30.471794",
     "exception": false,
     "start_time": "2025-04-20T16:05:00.479428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total embeddings generated: 101\n"
     ]
    }
   ],
   "source": [
    "from google.generativeai import embed_content\n",
    "\n",
    "# ✅ Use the newer model\n",
    "EMBED_MODEL = \"models/text-embedding-004\"\n",
    "\n",
    "def generate_embeddings(chunks):\n",
    "    embeddings = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        try:\n",
    "            response = embed_content(\n",
    "                model=EMBED_MODEL,\n",
    "                content=chunk,\n",
    "                task_type=\"retrieval_document\",  # or \"retrieval_query\"\n",
    "            )\n",
    "            embeddings.append(response[\"embedding\"])\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error embedding chunk {i}: {e}\")\n",
    "            embeddings.append(None)\n",
    "    return embeddings\n",
    "\n",
    "# Assuming full_text is already loaded from your PDF extraction\n",
    "text_chunks = split_text(full_text)  # Generate the chunks\n",
    "\n",
    "# Call the function to generate embeddings\n",
    "embeddings = generate_embeddings(text_chunks)\n",
    "\n",
    "# Print the number of valid embeddings\n",
    "valid_embeddings = [e for e in embeddings if e is not None]\n",
    "print(f\"✅ Total embeddings generated: {len(valid_embeddings)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc90e36",
   "metadata": {
    "papermill": {
     "duration": 0.007052,
     "end_time": "2025-04-20T16:05:30.486963",
     "exception": false,
     "start_time": "2025-04-20T16:05:30.479911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ****Store Embeddings in ChromaD****\n",
    "Initializes ChromaDB client and stores the text chunks with their embeddings into a named collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e48c72c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:05:30.502591Z",
     "iopub.status.busy": "2025-04-20T16:05:30.502321Z",
     "iopub.status.idle": "2025-04-20T16:05:31.138366Z",
     "shell.execute_reply": "2025-04-20T16:05:31.137392Z"
    },
    "papermill": {
     "duration": 0.645713,
     "end_time": "2025-04-20T16:05:31.139762",
     "exception": false,
     "start_time": "2025-04-20T16:05:30.494049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stored 101 chunks in ChromaDB collection.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# Step 1: Initialize ChromaDB client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Step 2: Create a new collection (or get if it already exists)\n",
    "collection = chroma_client.create_collection(name=\"study_notes2\")\n",
    "\n",
    "# Step 3: Prepare data for insertion\n",
    "ids = [f\"chunk-{i}\" for i in range(len(text_chunks))]\n",
    "\n",
    "collection.add(\n",
    "    documents=text_chunks,     # Original chunks\n",
    "    embeddings=embeddings,     # Generated embeddings\n",
    "    ids=ids                    # Unique IDs\n",
    ")\n",
    "\n",
    "print(f\"✅ Stored {len(text_chunks)} chunks in ChromaDB collection.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ed58f",
   "metadata": {
    "papermill": {
     "duration": 0.007186,
     "end_time": "2025-04-20T16:05:31.154514",
     "exception": false,
     "start_time": "2025-04-20T16:05:31.147328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ****List Available Gemini Models****\n",
    "Prints the list of available Gemini models that support content generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "676fefb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:05:31.170486Z",
     "iopub.status.busy": "2025-04-20T16:05:31.170159Z",
     "iopub.status.idle": "2025-04-20T16:05:31.369853Z",
     "shell.execute_reply": "2025-04-20T16:05:31.368716Z"
    },
    "papermill": {
     "duration": 0.209444,
     "end_time": "2025-04-20T16:05:31.371406",
     "exception": false,
     "start_time": "2025-04-20T16:05:31.161962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ models/gemini-1.0-pro-vision-latest supports content generation\n",
      "✅ models/gemini-pro-vision supports content generation\n",
      "✅ models/gemini-1.5-pro-latest supports content generation\n",
      "✅ models/gemini-1.5-pro-001 supports content generation\n",
      "✅ models/gemini-1.5-pro-002 supports content generation\n",
      "✅ models/gemini-1.5-pro supports content generation\n",
      "✅ models/gemini-1.5-flash-latest supports content generation\n",
      "✅ models/gemini-1.5-flash-001 supports content generation\n",
      "✅ models/gemini-1.5-flash-001-tuning supports content generation\n",
      "✅ models/gemini-1.5-flash supports content generation\n",
      "✅ models/gemini-1.5-flash-002 supports content generation\n",
      "✅ models/gemini-1.5-flash-8b supports content generation\n",
      "✅ models/gemini-1.5-flash-8b-001 supports content generation\n",
      "✅ models/gemini-1.5-flash-8b-latest supports content generation\n",
      "✅ models/gemini-1.5-flash-8b-exp-0827 supports content generation\n",
      "✅ models/gemini-1.5-flash-8b-exp-0924 supports content generation\n",
      "✅ models/gemini-2.5-pro-exp-03-25 supports content generation\n",
      "✅ models/gemini-2.5-pro-preview-03-25 supports content generation\n",
      "✅ models/gemini-2.5-flash-preview-04-17 supports content generation\n",
      "✅ models/gemini-2.0-flash-exp supports content generation\n",
      "✅ models/gemini-2.0-flash supports content generation\n",
      "✅ models/gemini-2.0-flash-001 supports content generation\n",
      "✅ models/gemini-2.0-flash-exp-image-generation supports content generation\n",
      "✅ models/gemini-2.0-flash-lite-001 supports content generation\n",
      "✅ models/gemini-2.0-flash-lite supports content generation\n",
      "✅ models/gemini-2.0-flash-lite-preview-02-05 supports content generation\n",
      "✅ models/gemini-2.0-flash-lite-preview supports content generation\n",
      "✅ models/gemini-2.0-pro-exp supports content generation\n",
      "✅ models/gemini-2.0-pro-exp-02-05 supports content generation\n",
      "✅ models/gemini-exp-1206 supports content generation\n",
      "✅ models/gemini-2.0-flash-thinking-exp-01-21 supports content generation\n",
      "✅ models/gemini-2.0-flash-thinking-exp supports content generation\n",
      "✅ models/gemini-2.0-flash-thinking-exp-1219 supports content generation\n",
      "✅ models/learnlm-1.5-pro-experimental supports content generation\n",
      "✅ models/learnlm-2.0-flash-experimental supports content generation\n",
      "✅ models/gemma-3-1b-it supports content generation\n",
      "✅ models/gemma-3-4b-it supports content generation\n",
      "✅ models/gemma-3-12b-it supports content generation\n",
      "✅ models/gemma-3-27b-it supports content generation\n"
     ]
    }
   ],
   "source": [
    "from google.generativeai import list_models\n",
    "\n",
    "for model in list_models():\n",
    "    if \"generateContent\" in model.supported_generation_methods:\n",
    "        print(f\"✅ {model.name} supports content generation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98cb133",
   "metadata": {
    "papermill": {
     "duration": 0.007335,
     "end_time": "2025-04-20T16:05:31.386637",
     "exception": false,
     "start_time": "2025-04-20T16:05:31.379302",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ****Ask Questions Using Gemini Pro****\n",
    "Sends a context + question to the Gemini model and generates a relevant, concise answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7eed5e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:05:31.402551Z",
     "iopub.status.busy": "2025-04-20T16:05:31.402209Z",
     "iopub.status.idle": "2025-04-20T16:05:32.496870Z",
     "shell.execute_reply": "2025-04-20T16:05:32.495968Z"
    },
    "papermill": {
     "duration": 1.104501,
     "end_time": "2025-04-20T16:05:32.498448",
     "exception": false,
     "start_time": "2025-04-20T16:05:31.393947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Answer: LLM stands for Large Language Model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "# 🔑 Fetch the API key securely\n",
    "\n",
    "GOOGLE_API_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "# ✅ Configure Gemini with your actual key\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Load the model\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "\n",
    "# Sample input\n",
    "question = \"What is an LLM?\"\n",
    "context = \"\"\"Generative AI, and LLMs specifically, is a General Purpose Technology...\"\"\"\n",
    "\n",
    "# Generate response\n",
    "response = model.generate_content(\n",
    "    f\"\"\"Answer the following question based on the context below:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer in a short and clear way.\"\"\"\n",
    ")\n",
    "\n",
    "# Output\n",
    "print(\"📘 Answer:\", response.text.strip())\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 97258,
     "sourceType": "competition"
    },
    {
     "datasetId": 7168585,
     "sourceId": 11443191,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 101.114858,
   "end_time": "2025-04-20T16:05:35.776721",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-20T16:03:54.661863",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
